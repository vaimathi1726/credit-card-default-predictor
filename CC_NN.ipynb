{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 1) Define the “base” estimator\n",
    "base_clf = MLPClassifier(\n",
    "    max_iter=2000,      # enough epochs to converge\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2) Set up a param_grid just like before—include both your feature-engineer keys\n",
    "#    *and* the MLP-specific hyperparameters:\n",
    "param_grid = {\n",
    "    'feature_method':     [None, 'polynomial', 'pca', 'rbf'],\n",
    "    'degree':             [2, 3],           # for polynomial\n",
    "    'n_components':       [5, 10],          # for PCA\n",
    "    'gamma':              [0.1, 0.5],       # for RBF transform\n",
    "\n",
    "    # --- MLPClassifier hyperparams ---\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50,50)],\n",
    "    'activation':         ['relu', 'tanh'],    # non-linearities\n",
    "    'alpha':              [1e-4, 1e-3],        # L2 penalty (weight decay)\n",
    "    'learning_rate_init': [1e-3, 1e-2],        # initial learning rate\n",
    "}\n",
    "\n",
    "# 3) Run your grid search / eval\n",
    "results_nn = grid_evaluate(\n",
    "    estimator  = base_clf,\n",
    "    param_grid = param_grid,\n",
    "    X_train    = X_train,\n",
    "    X_test     = X_test,\n",
    "    y_train    = y_train,\n",
    "    y_test     = y_test\n",
    ")\n",
    "\n",
    "# 4) Inspect best model by F1\n",
    "best_nn = results_nn.sort_values('f1_score', ascending=False)\n",
    "print(best_nn.iloc[0])\n",
    "print(best_nn)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
